The AllReduce approach relies on direct communication between the devices to mutually exchange model gradients and parameters by using communication primitives. It involves aggregating data from devices and redistributing the aggregated results back to them. With this synchronization, the nodes collaboratively update the model parameters in a consistent manner. The approach became popular after Horovod demonstrated that the so-called ring all-reduce variant of this algorithm was able to reach nearly 90 percent scaling efficiency when training large CNN models on 128 GPUs.